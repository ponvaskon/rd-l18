# GenAI Music and Video Generator

## Project Goals:
The project aims to create a generative AI solution that takes user input (genre, duration, and a video prompt) to generate a video with audio. The audio is generated using **Meta MusicGen**, and the video is generated using **LTX-Video** (a text-to-video model). The final output combines the generated music with the generated video and serves it through a Flask API.

## Instructions for Launching:

1. **Clone the Repository**:
   First, clone the repository to your local machine or Google Colab.
   ```bash
   git clone https://github.com/your-username/my-genai-project.git
   cd my-genai-project

2. **Install Dependencies: Install the required libraries using pip.**
pip install -r requirements.txt

3. **Run the Application: Launch the Flask app by running the following command:**
python src/app.py

4. **Start ngrok:** ngrok will provide a public URL for the API. The Flask app will run on http://localhost:5000, and ngrok will provide a public URL, e.g., http://<your-ngrok-url>.ngrok.io.

5. Use the API: You can use the /generate_content API to generate content. Send a POST request with the following parameters:

genre: The genre of music (e.g., "pop", "rock").
duration: Duration of the music and video in seconds.
video_prompt: Text prompt to generate the video.

Example:

{
    "genre": "pop",
    "duration": 15,
    "video_prompt": "A beautiful sunset over the ocean."
}

The API will return a video file with the music you generated.

**Description of the Models and Algorithms:**

**Music Generation:**
Meta MusicGen is used for generating music based on a genre text prompt. The MusicGen model is a pre-trained model from Meta, designed to generate high-quality music with a specific genre and duration based on the user's input.
The audio is generated by calling model.generate() method from the MusicGen library.

**Video Generation:**
LTX-Video is a text-to-video generation model that uses a diffusion-based pipeline to create high-quality video frames from a given text prompt. In this project, we use LTX-Video to generate frames that are then assembled into a video.
The frames are generated using pipe() method of the LTX-Video model.
The frames are then combined using moviepy to create a video.

**Combining Music and Video:**
Once both music and video are generated, moviepy is used to combine the generated audio and video into a final video file.
The final video is returned to the user through the Flask API.

**Requirements:**
Python 3.7 or later
Flask
Meta MusicGen
LTX-Video
Moviepy
Diffusers
Pyngrok
Torch
Torchaudio
Soundfile

Install all dependencies:
pip install -r requirements.txt

**Challenges and Solutions:**
Model Compatibility: The project requires multiple models for generating music and video. We used Meta MusicGen for music and LTX-Video for video generation. Some challenges included ensuring the models work seamlessly together.
Generating Video with the Correct Length: The video duration is controlled based on the number of frames generated for the video. The duration is calculated by multiplying the video length (in seconds) by the number of frames per second (24 fps).
Handling Model Incompatibilities: The code was adjusted to ensure compatibility between the different models (music and video generation). Using Hugging Face models for video generation and Meta MusicGen for audio generation posed no significant issues when proper model loading and parameter settings were handled.

**Future Work:**
Improvement in Video Quality: Implement higher-resolution video generation models.
Additional Audio Features: Provide options for the user to specify mood, tempo, or instruments in the music generation.
Interactive User Interface: Integrate a frontend for easier user interaction with the system.


### Notes for Use:
- **Replace** `your-username` in the GitHub repository URL with your GitHub username when you push it to your GitHub account.
- Ensure your **ngrok auth token** is set correctly for accessing the API from a public URL.
